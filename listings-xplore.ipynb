{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596721106807",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['- Software Scientist-MICCoM - Lemont IL 60439.txt',\n 'ArmyDataScientist.txt',\n 'BrookhavenDetectorHardware.txt',\n 'BrookhavenMathData.txt',\n 'BrookhavenQuantumCompSci.txt',\n 'BrookhavenSoftwareDevSci.txt',\n 'CIAdatascientist.txt',\n 'Data Scientist - Early Career in Livermore, CA - Careers at Lawrence Livermore National Laboratory.txt',\n 'Data Scientist in Livermore, CA - Careers at Lawrence Livermore National Laboratory.txt',\n 'GeospatialBigData.txt',\n 'INLMetallurgist.txt',\n 'LBerkeleyBioCryst.txt',\n 'LBerkeleyCompChem.txt',\n 'LBerkeleyDataSciDNA.txt',\n 'LBerkeleyMachLearnBehavior.txt',\n 'LosAlamosWIPP.txt',\n 'Pacific Northwest National Laboratory Jobs - Data Scientist - Statistics, Early Career in RICHLAND, Washington, United States.txt',\n 'PrincetonFusionBigData.txt',\n 'R&D Staff Member- Used Nuclear Fuel Disposition.txt',\n 'RobertHalf.txt',\n 'Sandia1.txt',\n 'Sandia2.txt',\n 'Sandia3.txt',\n 'Sandia4.txt',\n 'SLAC-Xraylaser.txt',\n 'Technical Professional in Remote Sensing.txt',\n 'USGSCoastalDataScientist.txt',\n 'YelpDataAnalyst.txt',\n 'YelpDataScientist.txt']"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "os.listdir('jobs-archive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could easily be worth eliminating all the digits prior to processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denumerate(inp_doc):\n",
    "    inp_strs = inp_doc.split()\n",
    "    new_strs = []\n",
    "    for inp_str in inp_strs:\n",
    "        new_strs.append(''.join(char for char in inp_str if char.isalpha()))\n",
    "    return ' '.join(new_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Skip Navigation pcrtgmain giestingalumnindedu Update Profile Information httpscareerspeopleclickcomc\n"
    }
   ],
   "source": [
    "listings = []\n",
    "folder = 'jobs-archive/'\n",
    "for filename in os.listdir(folder):\n",
    "    with open(folder+filename,'r',errors='ignore') as listing:\n",
    "        listings.append(denumerate(listing.read()))\n",
    "print(listings[0][0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a _bit_ aggressive but we're just playing today. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(29, 3280)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "doc_word = vectorizer.fit_transform(listings)\n",
    "doc_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   abbreviated  abilities  ability  able  abreast  absorption  abstract  \\\n0            0          0        2     0        0           0         0   \n1            0          2        0     0        0           0         0   \n2            0          2        2     0        0           0         0   \n3            0          1        3     0        0           0         0   \n4            0          2        2     0        0           0         0   \n\n   abuse  academia  academic  ...  youtube  youâll  âœaboutâ  âœauthenticityâ  \\\n0      0         0         0  ...        0       0         0                0   \n1      0         0         1  ...        0       0         0                0   \n2      0         0         0  ...        0       0         0                0   \n3      0         0         0  ...        0       0         0                0   \n4      0         1         0  ...        0       0         0                0   \n\n   âœautomate  âœplaying  âœworldâs  ïa  ïdevelop  ïexperience  \n0           0          0          0   0         0            0  \n1           0          0          0   0         0            0  \n2           0          0          0   0         0            0  \n3           0          0          0   0         0            0  \n4           0          0          0   0         0            0  \n\n[5 rows x 3280 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>abbreviated</th>\n      <th>abilities</th>\n      <th>ability</th>\n      <th>able</th>\n      <th>abreast</th>\n      <th>absorption</th>\n      <th>abstract</th>\n      <th>abuse</th>\n      <th>academia</th>\n      <th>academic</th>\n      <th>...</th>\n      <th>youtube</th>\n      <th>youâll</th>\n      <th>âœaboutâ</th>\n      <th>âœauthenticityâ</th>\n      <th>âœautomate</th>\n      <th>âœplaying</th>\n      <th>âœworldâs</th>\n      <th>ïa</th>\n      <th>ïdevelop</th>\n      <th>ïexperience</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 3280 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df = pd.DataFrame(doc_word.toarray(),\n",
    "    columns=vectorizer.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's certainly better, although there are encoding problems. There ought to be a routine to force accented letters to their base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.14828658, 0.18929004, 0.11626702])"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "lsa = TruncatedSVD(3)\n",
    "doc_topic = lsa.fit_transform(doc_word)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought the explained variances would come out in descending order."
   ]
  }
 ]
}